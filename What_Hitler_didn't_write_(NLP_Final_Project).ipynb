{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading file. Dataset is the book Mein Kampf by Hitler. Accessed from Project Gutenberg site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTRODUCTION\n",
      "\n",
      "\n",
      "\n",
      "AUTHOR'S PREFACE\n",
      "\n",
      "On April 1st, 1924, I began to serve my sentence of detention in the\n",
      "Fortress of Landsberg am Lech, following the verdict of the Munich\n",
      "People's Court of that time.\n",
      "\n",
      "After years of uninterrupted labour it was now possible for the first\n",
      "time to begin a work which many had asked for and which I myself felt\n",
      "would be profitable for the Movement. So I decided to devote two volumes\n",
      "to a description not only of the aims of our Movement but also of its\n",
      "development. Ther\n"
     ]
    }
   ],
   "source": [
    "filename = r'C:\\Users\\dell\\Desktop\\Mein_Kampf.txt'\n",
    "file=open(filename,'r')\n",
    "doc=file.read()\n",
    "file.close()\n",
    "print(doc[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to print concordances of a word say, 'Jew'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'concordance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e00363707f76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcordance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Jew\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'concordance'"
     ]
    }
   ],
   "source": [
    "doc.concordance(\"Jew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to load directly from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xfc in position 10426: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5bea54fe3059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://gutenberg.net.au/ebooks02/0200601.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xfc in position 10426: invalid start byte"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://gutenberg.net.au/ebooks02/0200601.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8') #incorrect encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://gutenberg.net.au/ebooks02/0200601.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('iso8859')\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'concordance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4096591f4546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcordance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Jew\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'concordance'"
     ]
    }
   ],
   "source": [
    "raw.concordance(\"Jew\")\n",
    "#concordance is a function under nltk.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3_FINAL\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: 'U' mode is deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "f=open(r'C:\\Users\\dell\\Desktop\\Meine_Kampf.txt','rU')\n",
    "raw1=f.read()\n",
    "#reading text and using nltk tokens to form text so that we can find concordances\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 211 matches:\n",
      "rticularly unpleasant for me . In the Jew I still saw only a man who was of a d\n",
      "ks . My first thought was : Is this a Jew ? They certainly did not have this ap\n",
      "cultural life , in which at least one Jew did not participate ? On putting the \n",
      "aggot in a putrescent body , a little Jew who was often blinded by the sudden l\n",
      "blooded , thick-skinned and shameless Jew who showed his consummate skill in co\n",
      " . But as I learned to track down the Jew in all the different spheres of cultu\n",
      " at last to know for certain that the Jew is not a German . Thus I finally disc\n",
      " must be devoted to such work . But a Jew can never be rescued from his fixed n\n",
      "re for you on the following day . The Jew would be utterly oblivious to what ha\n",
      "nt ! No . The more I came to know the Jew , the easier it was to excuse the wor\n",
      " I now understood the language of the Jew . I realized that the Jew uses langua\n",
      "uage of the Jew . I realized that the Jew uses language for the purpose of diss\n",
      " would finally disappear . Should the Jew , with the aid of his Marxist creed ,\n",
      "eator . In standing guard against the Jew I am defending the handiwork of the L\n",
      "ways done and always will do . Only a Jew can praise an institution which is as\n",
      "ectively ; but you could never find a Jew who took a similar attitude towards h\n",
      "ould settle the matter , hereupon the Jew could still carry on his business saf\n",
      "lves were being led by the nose . The Jew readily adjusted himself to this form\n",
      "is the solvent of human society , the Jew , here and there and everywhere -- th\n",
      "icious mentality among our people the Jew is always in the first line . He know\n",
      "s the cook is one and the same -- the Jew . One should be careful about contrad\n",
      "ed by Jews . Almost every clerk was a Jew and every Jew was a clerk . I was ama\n",
      "lmost every clerk was a Jew and every Jew was a clerk . I was amazed at this mu\n",
      "e and purpose of the life-work of the Jew , Karl Marx . His CAPITAL became inte\n",
      "true . He ( Schopenhauer ) called the Jew `` The Great Master of Lies '' . Thos\n"
     ]
    }
   ],
   "source": [
    "text.concordance(\"Jew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for generating some random text. However, the generate() method is not available in NLTK 3.0 but will be reinstated in a subsequent version.\n",
    "#check https://github.com/nltk/nltk/issues/736\n",
    "text.generate(words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning up document\n",
    "doc = doc.replace('--', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens=word_tokenize(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations\n",
    "tokens = [' ' if w in string.punctuation else w for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing to lower case\n",
    "tokens = [word.lower() for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " \"'s\",\n",
       " '1924',\n",
       " '1st',\n",
       " 'a',\n",
       " 'about',\n",
       " 'after',\n",
       " 'aims',\n",
       " 'also',\n",
       " 'am',\n",
       " 'and',\n",
       " 'any',\n",
       " 'april',\n",
       " 'are',\n",
       " 'as',\n",
       " 'asked',\n",
       " 'author',\n",
       " 'be',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'belong',\n",
       " 'but',\n",
       " 'by',\n",
       " 'circulated',\n",
       " 'court',\n",
       " 'decided',\n",
       " 'describing',\n",
       " 'description',\n",
       " 'destroy',\n",
       " 'detention',\n",
       " 'development',\n",
       " 'devote',\n",
       " 'doctrinaire',\n",
       " 'fabrications',\n",
       " 'far',\n",
       " 'felt',\n",
       " 'fewer',\n",
       " 'first',\n",
       " 'followers',\n",
       " 'following',\n",
       " 'for',\n",
       " 'fortress',\n",
       " 'from',\n",
       " 'given',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'hearts',\n",
       " 'i',\n",
       " 'in',\n",
       " 'introduction',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'jewish',\n",
       " 'know',\n",
       " 'labour',\n",
       " 'landsberg',\n",
       " 'learned',\n",
       " 'lech',\n",
       " 'legendary',\n",
       " 'many',\n",
       " 'me',\n",
       " 'more',\n",
       " 'movement',\n",
       " 'munich',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'necessary',\n",
       " 'not',\n",
       " 'now',\n",
       " 'of',\n",
       " 'on',\n",
       " 'only',\n",
       " 'opportunity',\n",
       " 'our',\n",
       " 'over',\n",
       " 'own',\n",
       " 'people',\n",
       " 'possible',\n",
       " 'preface',\n",
       " 'press',\n",
       " 'profitable',\n",
       " 'profoundly',\n",
       " 'purely',\n",
       " 'second',\n",
       " 'sentence',\n",
       " 'serve',\n",
       " 'so',\n",
       " 'strangers',\n",
       " 'study',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'this',\n",
       " 'those',\n",
       " 'time',\n",
       " 'to',\n",
       " 'treatise',\n",
       " 'turn',\n",
       " 'two',\n",
       " 'understanding',\n",
       " 'uninterrupted',\n",
       " 'verdict',\n",
       " 'volume',\n",
       " 'volumes',\n",
       " 'was',\n",
       " 'well',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whose',\n",
       " 'wish',\n",
       " 'won',\n",
       " 'work',\n",
       " 'would',\n",
       " 'written',\n",
       " 'years'}"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing unique tokens\n",
    "set(tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1443"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding number of unique tokens\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5497"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking total number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2625068219028561"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding lexical diversity\n",
    "len(set(tokens))/float(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'author', \"'s\", 'preface', 'on', 'april', '1st', ' ', '1924', ' ', 'i', 'began', 'to', 'serve', 'my', 'sentence', 'of', 'detention', 'in', 'the', 'fortress', 'of', 'landsberg', 'am', 'lech', ' ', 'following', 'the', 'verdict', 'of', 'the', 'munich', 'people', \"'s\", 'court', 'of', 'that', 'time', ' ', 'after', 'years', 'of', 'uninterrupted', 'labour', 'it', 'was', 'now', 'possible', 'for', 'the', 'first', 'time', 'to', 'begin', 'a', 'work', 'which', 'many', 'had', 'asked', 'for', 'and', 'which', 'i', 'myself', 'felt', 'would', 'be', 'profitable', 'for', 'the', 'movement', ' ', 'so', 'i', 'decided', 'to', 'devote', 'two', 'volumes', 'to', 'a', 'description', 'not', 'only', 'of', 'the', 'aims', 'of', 'our', 'movement', 'but', 'also', 'of', 'its', 'development', ' ', 'there', 'is', 'more', 'to', 'be', 'learned', 'from', 'this', 'than', 'from', 'any', 'purely', 'doctrinaire', 'treatise', ' ', 'this', 'has', 'also', 'given', 'me', 'the', 'opportunity', 'of', 'describing', 'my', 'own', 'development', 'in', 'so', 'far', 'as', 'such', 'a', 'description', 'is', 'necessary', 'to', 'the', 'understanding', 'of', 'the', 'first', 'as', 'well', 'as', 'the', 'second', 'volume', 'and', 'to', 'destroy', 'the', 'legendary', 'fabrications', 'which', 'the', 'jewish', 'press', 'have', 'circulated', 'about', 'me', ' ', 'in', 'this', 'work', 'i', 'turn', 'not', 'to', 'strangers', 'but', 'to', 'those', 'followers', 'of', 'the', 'movement', 'whose', 'hearts', 'belong', 'to', 'it', 'and', 'who', 'wish', 'to', 'study', 'it', 'more', 'profoundly', ' ', 'i', 'know', 'that', 'fewer', 'people', 'are', 'won', 'over', 'by', 'the', 'written']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5396\n"
     ]
    }
   ],
   "source": [
    "#generating sequences of words that are of length 101 from the tokens. \n",
    "#Each sequence is space separated and saved in line. Finally appended to a list called sequences\n",
    "length = 100 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving sequences to a file\n",
    "data = '\\n'.join(sequences)\n",
    "file = open(\"MKSeq.txt\", 'w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"MKSeq.txt\", 'r')\n",
    "\n",
    "text_seq = file.read()\n",
    "file.close()\n",
    "lines = text_seq.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction preface on april i began to serve my sentence of detention in the fortress of landsberg am following the verdict of the munich court of that after years of uninterrupted labour it was now possible for the first time to begin a work which many had asked for and which i myself felt would be profitable for the so i decided to devote two volumes to a description not only of the aims of our movement but also of its there is more to be learned from this than from any purely doctrinaire th\n"
     ]
    }
   ],
   "source": [
    "print(text_seq[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each unique word must be mapped to an integer  because Keras layers only work on numbers. \n",
    "#Thus Tokenizer is used to convert input to sequence of integers.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(filters='\\n')\n",
    "tokenizer.fit_on_texts(lines) #training and fitting instance tokenizer on lines(text)\n",
    "sequences = tokenizer.texts_to_sequences(lines) #encodes all training sequences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran into an error--\n",
    "IndexError: too many indices for array when trying to divide array into X and y \n",
    "\n",
    "Possible cause:input is not even and np.array doesnâ€™t parse it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solved using the following\n",
    "original_sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "aligned_sequneces = []\n",
    "for sequence in original_sequences:\n",
    "    aligned_sequence = np.zeros(5401, dtype=np.int64)\n",
    "    aligned_sequence[:len(sequence)] = np.array(sequence, dtype=np.int64)\n",
    "    aligned_sequneces.append(aligned_sequence)\n",
    "\n",
    "sequences = np.array(aligned_sequneces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 440 1202   20 ...    0    0    0]\n",
      " [1202   20 1200 ...    0    0    0]\n",
      " [  20 1200   13 ...    0    0    0]\n",
      " ...\n",
      " [  76   16  281 ...    0    0    0]\n",
      " [  16  281    1 ...    0    0    0]\n",
      " [ 281    1  115 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4306,)\n",
      "100\n",
      "(4306, 101)\n",
      "(4306, 100)\n",
      "(4306, 1202)\n"
     ]
    }
   ],
   "source": [
    "#Separating data to X and y\n",
    "#X contains all sequences with all words except the last\n",
    "#y contains all sequences with only the last words\n",
    "#doing one hot encoding on y so that model can predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "print(y.shape)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "print(seq_length)\n",
    "print(sequences.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1798 2716 2521 ... 1516 2987  196]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#trying one-hot encoding using sklearn libraries\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(lines)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4306, 4306)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 5400, 50)          72250     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 5400, 100)         60400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1445)              145945    \n",
      "=================================================================\n",
      "Total params: 369,095\n",
      "Trainable params: 369,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#defining model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5401/5401 [==============================] - 1634s 302ms/step - loss: 2.9219 - accuracy: 0.9763:22 - loss: 3.8516  - ETA: 45s - loss: 3.0071 - accuracy: 0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e9a4eae748>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling model by defining which loss function, optimizer and metrics to use to reduce loss\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fitting model\n",
    "model.fit(X, y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model checkpoints\n",
    "model.save('nlp.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-72d8b2971d33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mseed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_text\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "#Generating text by first selecting a random line of text from the input text\n",
    "from random import randint\n",
    "from pickle import load\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eda2796a3844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mseed_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'seed_text' is not defined"
     ]
    }
   ],
   "source": [
    "seed_text.word_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding to sequnce of integer using same tokenizer instance as before\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting list to array\n",
    "arr = np.array(encoded) \n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([217,  10,   9,   7,  71,   6, 191, 731,   5,  19, 218,   6, 732,\n",
       "         4,   6, 133,   2, 173,  40,   9, 189,  17,  65,   9, 733,   4,\n",
       "        39,  68, 338,  19, 734,  51,  10, 735,   2,   1,  13,  73, 163,\n",
       "         3, 117, 221,   4,   1, 339,  18,  89, 736, 737,   5,  18,  52,\n",
       "        19, 340,  22, 738,   3, 138,  15,  37, 222, 739,   2,   1,  11,\n",
       "       223,   9, 123,   3, 740,   3,   1,  36,  61,   5, 223,   3,   1,\n",
       "        11,  53,  10, 224,  46, 741,  13,   4, 742,   1, 341,  18,   9,\n",
       "       743,   4,  23, 342, 343,  10,   1, 744, 120,  17])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding array with 5299 163's (the mean) because otherwise there is an input shape mismatch\n",
    "# seed text is only of length 101 but model expects a vector of shape (5400,)\n",
    "arr=np.pad(arr, (5000,299), 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[217  10   9   7  71   6 191 731   5  19 218   6 732   4   6 133   2 173\n",
      "  40   9 189  17  65   9 733   4  39  68 338  19 734  51  10 735   2   1\n",
      "  13  73 163   3 117 221   4   1 339  18  89 736 737   5  18  52  19 340\n",
      "  22 738   3 138  15  37 222 739   2   1  11 223   9 123   3 740   3   1\n",
      "  36  61   5 223   3   1  11  53  10 224  46 741  13   4 742   1 341  18\n",
      "   9 743   4  23 342 343  10   1 744 120  17]\n"
     ]
    }
   ],
   "source": [
    "# Padding with zeros instead of the mean\n",
    "arr1=np.pad(arr, (5000,299),'constant')\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5400,)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1.shape #the zero padded array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5400)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing array to matrix\n",
    "mat=np.asmatrix(arr)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5400)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to matrix the zero padded vector\n",
    "mat1=np.asmatrix(arr1)\n",
    "mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_4_input to have shape (5400,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-329-1bf75f5bc24a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3_FINAL\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_FINAL\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_FINAL\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_FINAL\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_4_input to have shape (5400,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#The error if no padding used and list fed directly\n",
    "yhat = model.predict_classes(encoded, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solved the problem\n",
    "yhat = model.predict_classes(mat, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gave way to hopes that were better suited to my browsing through my i chanced to come across some publications that dealt with military one of these publications was a popular history of the war of it consisted of two volumes of an illustrated periodical dating from those these became my favourite in a little while that great and heroic conflict began to take first place in my and from that time onwards i became more and more enthusiastic about everything that was in any way connected with war or military but this story of the war had a special significance\n",
      "\n",
      "                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "#method to generate the text using mean padded matrix\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    \n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        np.ravel(encoded)\n",
    "\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "# append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "october born march claus born august theodor von councillor to the superior provincial born may retired cavalry born may max erwin of born january lorenz ritter born march born october national officials refused to allow the dead heroes a common so i dedicate the first volume of this work to them as a common that the memory of those martyrs may be a permanent source of light for the followers of our the landsberg october introduction in placing before the reader this unabridged translation of adolf mein i feel it my duty to call attention to certain historical facts which must\n",
      "\n",
      "                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "#method to generate the text using zero padded matrix\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    \n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        np.ravel(encoded)\n",
    "        #predicting probabilities of next word\n",
    "        yhat = model.predict_classes(mat1, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat: #matched integer to word\n",
    "                out_word = word\n",
    "                break\n",
    "# append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
